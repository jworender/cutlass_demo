{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d514fa2e",
   "metadata": {},
   "source": [
    "# LLE Case 1 — Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089dc47f",
   "metadata": {},
   "source": [
    "This notebook mirrors the original `Case_1_markdown.Rmd` analysis using Python only.\n",
    "Synthetic longitudinal data are generated, transformed, and modeled with the same logic\n",
    "that powered the R workflow so the entire case study can be executed from a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7003c",
   "metadata": {},
   "source": [
    "### Data-generation recap\n",
    "\n",
    "The Case 1 signals are designed to mimic noisy, partially correlated sensors:\n",
    "\n",
    "1. Each variable is a composite of randomized sine waves, so every channel drifts over time.\n",
    "2. All variables remain range bound to resemble physical measurements.\n",
    "3. Only seven of forty channels drive the target; their time-shifted threshold crossings must\n",
    "   all be satisfied to trigger an event.\n",
    "4. Every example flattens eleven time steps (`TM10…TM0`) for each sensor, yielding 440 features.\n",
    "\n",
    "The R scripts implemented this process inside `build_case_1_data.R`. Here we call the matching\n",
    "Python generator (`sensor_generate - commented.py`) so the same settings stay in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Literal, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136600a",
   "metadata": {},
   "source": [
    "### Modeling helpers\n",
    "\n",
    "The next cell copies the reusable routines from `case_1_simple_script_scikit_fast_v6.py`.\n",
    "They cover grouping, data rectification, evaluation helpers, and plotting utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper routines copied from case_1_simple_script_scikit_fast_v6.py\n",
    "RESP_COL = 'INDC'\n",
    "EXCLUDE_COLS = ['X', 'UIC', 'iDate', 'iDate.x', 'iDate.y', 'class', 'cols', 'year']\n",
    "\n",
    "def timestamp(msg=None):\n",
    "    now = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    print(f\"{msg+': ' if msg else ''}{now}\")\n",
    "\n",
    "def organize(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
    "    feats = [c for c in df.columns if c not in EXCLUDE_COLS and c != RESP_COL]\n",
    "    groups: Dict[str, List[str]] = {}\n",
    "    for feat in feats:\n",
    "        prefix = re.sub(r'\\d+$', '', feat)\n",
    "        groups.setdefault(prefix, []).append(feat)\n",
    "    for prefix, flist in groups.items():\n",
    "        groups[prefix] = sorted(\n",
    "            [f for f in flist if re.search(r'(\\d+)$', f)],\n",
    "            key=lambda x: int(re.search(r'(\\d+)$', x).group(1))\n",
    "        ) + [f for f in flist if not re.search(r'(\\d+)$', f)]\n",
    "    return groups\n",
    "\n",
    "def _flatten_group_order(groups: Dict[str, List[str]], present_cols: List[str]) -> List[str]:\n",
    "    ordered = []\n",
    "    present = set(present_cols)\n",
    "    for g in sorted(groups.keys()):\n",
    "        for f in groups[g]:\n",
    "            if f in present:\n",
    "                ordered.append(f)\n",
    "    remaining = [c for c in present_cols if c not in set(ordered) and c != RESP_COL and c not in EXCLUDE_COLS]\n",
    "    return ordered + remaining\n",
    "\n",
    "def _limits_from_training(groups: Dict[str, List[str]], features: List[str], rmin: np.ndarray, rmax: np.ndarray) -> Dict[str, Dict[str, Tuple[float, float]]]:\n",
    "    lims: Dict[str, Dict[str, Tuple[float, float]]] = {g: {} for g in groups}\n",
    "    f2i = {f:i for i,f in enumerate(features)}\n",
    "    for g, fl in groups.items():\n",
    "        for f in fl:\n",
    "            if f in f2i:\n",
    "                i = f2i[f]\n",
    "                lims[g][f] = (float(rmin[i]), float(rmax[i]))\n",
    "    return lims\n",
    "\n",
    "def _scores_array_from_lrcv(lrcv) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract fold-by-C scores and the C grid from a fitted LogisticRegressionCV(refit=False).\n",
    "    Returns:\n",
    "      scores_fc : shape (n_folds, n_Cs)     (higher is better since scoring='neg_log_loss')\n",
    "      Cs        : shape (n_Cs,)\n",
    "    Works for both ndarray and dict-of-ndarray variants of scikit's scores_.\n",
    "    \"\"\"\n",
    "    Cs = np.asarray(lrcv.Cs_, dtype=float)  # ascending\n",
    "    scores = lrcv.scores_\n",
    "    if isinstance(scores, dict):\n",
    "        # pick one class (binary => one entry)\n",
    "        scores_fc = next(iter(scores.values()))  # shape (n_folds, n_Cs)\n",
    "    else:\n",
    "        # already an array: (n_classes, n_folds, n_Cs) or (n_folds, n_Cs) depending on scikit version\n",
    "        arr = np.asarray(scores)\n",
    "        if arr.ndim == 3:\n",
    "            scores_fc = arr[0, :, :]  # take first class for binary\n",
    "        else:\n",
    "            scores_fc = arr           # (n_folds, n_Cs)\n",
    "    return scores_fc, Cs\n",
    "\n",
    "def _select_c_via_1se(\n",
    "    mean_losses: np.ndarray,\n",
    "    se_losses: np.ndarray,\n",
    "    Cs: np.ndarray\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Given mean_losses(C), se_losses(C) and the C grid (ascending),\n",
    "    pick C_1se = smallest C with mean_loss <= min_loss + min_se  (i.e., most-regularized within 1-SE).\n",
    "    Returns:\n",
    "      C_1se, C_min\n",
    "    \"\"\"\n",
    "    j_min = int(np.argmin(mean_losses))\n",
    "    thr = float(mean_losses[j_min] + se_losses[j_min])  # 1-SE threshold\n",
    "    # candidates whose mean loss within threshold\n",
    "    cand = np.where(mean_losses <= thr)[0]\n",
    "    j_1se = int(np.min(cand)) if cand.size else j_min  # most-regularized (smallest C => left-most index)\n",
    "    return float(Cs[j_1se]), float(Cs[j_min])\n",
    "\n",
    "def _fit_logregcv_1se_core(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    Cs: np.ndarray,\n",
    "    cv: int = 3,\n",
    "    tol: float = 1e-3,\n",
    "    max_iter: int = 2000,\n",
    "    penalty: str = \"l1\",\n",
    "    solver: str = \"saga\",\n",
    "    random_state: int = 42,\n",
    "    scoring: str = \"neg_log_loss\"\n",
    ") -> Tuple[LogisticRegression, dict]:\n",
    "    \"\"\"\n",
    "    Core: run LogisticRegressionCV(refit=False), compute 1-SE pick, then refit LogisticRegression at C_1se.\n",
    "    Returns:\n",
    "      final_estimator (refit on all data at C_1se),\n",
    "      info dict with arrays and choices.\n",
    "    \"\"\"\n",
    "    # 1) CV without refit\n",
    "    lrcv = LogisticRegressionCV(\n",
    "        Cs=Cs,\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        tol=tol,\n",
    "        max_iter=max_iter,\n",
    "        refit=False,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    lrcv.fit(X, y)\n",
    "\n",
    "    # 2) From scores (higher is better), make losses (lower is better)\n",
    "    scores_fc, Cs_grid = _scores_array_from_lrcv(lrcv)           # (folds, Cs)\n",
    "    mean_scores = scores_fc.mean(axis=0)\n",
    "    # Use ddof=1 if >=2 folds; else zero SE to avoid NaN\n",
    "    se_scores = (scores_fc.std(axis=0, ddof=1) / np.sqrt(scores_fc.shape[0])) if scores_fc.shape[0] > 1 else np.zeros_like(mean_scores)\n",
    "\n",
    "    mean_losses = -mean_scores\n",
    "    se_losses = se_scores  # SE of scores == SE of (-loss) up to sign; threshold uses addition so same values\n",
    "\n",
    "    C_1se, C_min = _select_c_via_1se(mean_losses, se_losses, Cs_grid)\n",
    "\n",
    "    # 3) Final refit at C_1se\n",
    "    final = LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        solver=solver,\n",
    "        C=float(C_1se),\n",
    "        tol=tol,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    final.fit(X, y)\n",
    "\n",
    "    info = dict(\n",
    "        Cs=Cs_grid,\n",
    "        mean_losses=mean_losses,\n",
    "        se_losses=se_losses,\n",
    "        C_min=C_min,\n",
    "        C_1se=C_1se,\n",
    "    )\n",
    "    return final, info\n",
    "\n",
    "def rectify_fast(\n",
    "    df: pd.DataFrame,\n",
    "    groups: Dict[str, List[str]],\n",
    "    limits: Optional[Dict[str, Dict[str, Tuple[float, float]]]] = None,\n",
    "    sdfilter: Optional[float] = 3.0,\n",
    "    snap: float = 0.001,\n",
    "):\n",
    "    feat_candidates = [c for c in df.columns if c not in EXCLUDE_COLS and c != RESP_COL]\n",
    "    features = _flatten_group_order(groups, feat_candidates)\n",
    "\n",
    "    X = df[features].to_numpy(dtype=np.float64, copy=False)\n",
    "    y = df[RESP_COL].to_numpy(dtype=bool, copy=False)\n",
    "\n",
    "    n, p = X.shape\n",
    "    if limits is None:\n",
    "        if not np.any(y):\n",
    "            rmin = np.full(p, np.nan, dtype=np.float64)\n",
    "            rmax = np.full(p, np.nan, dtype=np.float64)\n",
    "        else:\n",
    "            X_pos = X[y, :]\n",
    "            mu = np.nanmean(X_pos, axis=0)\n",
    "            sd = np.nanstd(X_pos, axis=0, ddof=0)\n",
    "            if sdfilter is not None:\n",
    "                low = mu - sdfilter * sd\n",
    "                high = mu + sdfilter * sd\n",
    "                mask = (X_pos > low) & (X_pos < high)\n",
    "                Xpf = np.where(mask, X_pos, np.nan)\n",
    "            else:\n",
    "                Xpf = X_pos\n",
    "            rmin = np.nanmin(Xpf, axis=0)\n",
    "            rmax = np.nanmax(Xpf, axis=0)\n",
    "            snap_count = snap * n\n",
    "            lower_counts = np.sum(X < rmin, axis=0)\n",
    "            upper_counts = np.sum(X > rmax, axis=0)\n",
    "            rmin = np.where(lower_counts < snap_count, np.nan, rmin)\n",
    "            rmax = np.where(upper_counts < snap_count, np.nan, rmax)\n",
    "        limits_dict = _limits_from_training(groups, features, rmin, rmax)\n",
    "    else:\n",
    "        rmin = np.full(p, np.nan, dtype=np.float64)\n",
    "        rmax = np.full(p, np.nan, dtype=np.float64)\n",
    "        for g, fl in groups.items():\n",
    "            for f in fl:\n",
    "                if f in df.columns and f in features:\n",
    "                    i = features.index(f)\n",
    "                    try:\n",
    "                        rmin[i], rmax[i] = limits[g][f]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        limits_dict = limits\n",
    "\n",
    "    left_fail  = (X < rmin) if np.isfinite(rmin).any() else np.zeros_like(X, dtype=bool)\n",
    "    right_fail = (X > rmax) if np.isfinite(rmax).any() else np.zeros_like(X, dtype=bool)\n",
    "    outside = left_fail | right_fail\n",
    "\n",
    "    vec = np.where(outside, -1, 1).astype(np.int8)\n",
    "    dnew = pd.DataFrame(vec, columns=features, index=df.index)\n",
    "    dnew[RESP_COL] = df[RESP_COL]\n",
    "    return dnew, limits_dict\n",
    "\n",
    "def sklearn_build_model_1se(\n",
    "    X_df: pd.DataFrame,\n",
    "    *,\n",
    "    y_col: str = \"INDC\",\n",
    "    use_scaler: bool = False,       # False for rectified ±1; True for raw\n",
    "    cv: int = 3,\n",
    "    cs: int = 15,\n",
    "    c_lo: float = -3.0,             # use (-4, +4) if you want parity with gd_v8\n",
    "    c_hi: float =  3.0,\n",
    "    tol: float = 1e-3,\n",
    "    max_iter: int = 2000,\n",
    "    solver: str = \"saga\",\n",
    "    random_state: int = 42\n",
    ") -> Tuple[Pipeline, dict]:\n",
    "    \"\"\"\n",
    "    Build a sklearn Pipeline (optional StandardScaler -> LogisticRegression at C_1se)\n",
    "    using the 1-SE rule over a logspace grid. Returns (pipeline, info).\n",
    "    The 'info' dict contains C_min, C_1se, Cs, mean_losses, se_losses.\n",
    "    \"\"\"\n",
    "    Xonly = X_df.drop(columns=[y_col]).to_numpy(dtype=np.float64, copy=False)\n",
    "    y     = X_df[y_col].astype(int).to_numpy()\n",
    "\n",
    "    Cs = np.logspace(c_lo, c_hi, cs).astype(float)\n",
    "\n",
    "    # Fit CV, compute 1-SE, refit LogisticRegression at C_1se\n",
    "    final_est, info = _fit_logregcv_1se_core(\n",
    "        Xonly, y,\n",
    "        Cs=Cs, cv=cv, tol=tol, max_iter=max_iter,\n",
    "        penalty=\"l1\", solver=solver, random_state=random_state,\n",
    "        scoring=\"neg_log_loss\"\n",
    "    )\n",
    "\n",
    "    steps = []\n",
    "    if use_scaler:\n",
    "        steps.append((\"scaler\", StandardScaler(with_mean=True, with_std=True)))\n",
    "    steps.append((\"lr\", final_est))\n",
    "    pipe = Pipeline(steps)\n",
    "    # NOTE: pipeline.fit here is unnecessary (final_est is already fit) but harmless if you prefer:\n",
    "    # pipe.fit(Xonly, y)\n",
    "    return pipe, info\n",
    "\n",
    "def sklearn_fit_1se_rectified(rt_train: pd.DataFrame, **kwargs) -> Tuple[Pipeline, dict]:\n",
    "    \"\"\"\n",
    "    1-SE model on rectified ±1 features (no scaler).\n",
    "    kwargs forwarded to sklearn_build_model_1se (cv, cs, c_lo/c_hi, tol, max_iter, solver, random_state).\n",
    "    \"\"\"\n",
    "    return sklearn_build_model_1se(rt_train, use_scaler=False, **kwargs)\n",
    "\n",
    "def sklearn_fit_1se_raw(raw_train: pd.DataFrame, **kwargs) -> Tuple[Pipeline, dict]:\n",
    "    \"\"\"\n",
    "    1-SE model on raw continuous features (with StandardScaler).\n",
    "    kwargs forwarded to sklearn_build_model_1se (cv, cs, c_lo/c_hi, tol, max_iter, solver, random_state).\n",
    "    \"\"\"\n",
    "    return sklearn_build_model_1se(raw_train, use_scaler=True, **kwargs)\n",
    "\n",
    "def evaluate(pipe: Pipeline, X: pd.DataFrame, y_col: str = RESP_COL):\n",
    "    Xonly = X.drop(columns=[y_col])\n",
    "    y = X[y_col].astype(int).to_numpy()\n",
    "    prob = pipe.predict_proba(Xonly.to_numpy(dtype=np.float64, copy=False))[:, 1]\n",
    "    auc = roc_auc_score(y, prob)\n",
    "    prec, rec, _ = precision_recall_curve(y, prob)\n",
    "    fscore = 2 * rec * prec / (rec + prec + 1e-9)\n",
    "    return prob, y.astype(bool), auc, fscore\n",
    "\n",
    "def plot_sorted(prob: np.ndarray, y_bool: np.ndarray, title: str, h: float = 0.5):\n",
    "    import matplotlib.pyplot as plt\n",
    "    order = np.argsort(prob)\n",
    "    fig = plt.figure(num=title)\n",
    "    ax = fig.gca()\n",
    "    sc = ax.scatter(np.arange(prob.size), prob[order], c=y_bool[order], s=6, alpha=0.6, rasterized=True)\n",
    "    ax.axhline(h, linestyle=\"--\", linewidth=2)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Sorted Examples\")\n",
    "    ax.set_ylabel(\"Model Response\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def coefficient_series_only(pipe: Pipeline, feature_names: List[str]):\n",
    "    \"\"\"\n",
    "    Return only the coefficients (no intercept) as a Series aligned to feature_names.\n",
    "    \"\"\"\n",
    "    lr = pipe.named_steps[\"lr\"]\n",
    "    coef = lr.coef_.ravel()\n",
    "    if coef.shape[0] != len(feature_names):\n",
    "        raise ValueError(f\"coef length {coef.shape[0]} != feature_names length {len(feature_names)}\")\n",
    "    return pd.Series(coef, index=feature_names)\n",
    "\n",
    "def plot_coeff_bars(beta: pd.Series, title: str, highlights: Optional[List[str]] = None,\n",
    "                    annotate: bool = True, fontsize: int = 9):\n",
    "    \"\"\"\n",
    "    Bar plot of coefficients (no intercept). If `highlights` is provided, annotate those\n",
    "    coefficients with the feature name at their bar location.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(num=title)\n",
    "    ax = fig.gca()\n",
    "    x = np.arange(len(beta))\n",
    "    bars = ax.bar(x, beta.values, linewidth=1)\n",
    "\n",
    "    # nice symmetric y-limits with a little headroom for labels\n",
    "    m = np.nanmax(np.abs(beta.values)) if len(beta) else 1.0\n",
    "    ylim = max(m * 1.3, 0.5)\n",
    "    ax.set_ylim(-0.1*ylim, ylim)\n",
    "\n",
    "    # Annotate highlighted feature names\n",
    "    if highlights:\n",
    "        # For each pattern, find feature(s) that endwith the pattern and label\n",
    "        for pat in highlights:\n",
    "            matches = [i for i, nm in enumerate(beta.index) if nm.endswith(pat)]\n",
    "            for i in matches:\n",
    "                name = beta.index[i]\n",
    "                val  = beta.values[i]\n",
    "                # place label slightly above or below the bar depending on sign\n",
    "                offset = 0.04 * ylim\n",
    "                y = val + (offset if val >= 0 else -offset)\n",
    "                va = 'bottom' if val >= 0 else 'top'\n",
    "                ax.text(i, y, name, rotation=90, ha='center', va=va, fontsize=fontsize)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4d5832",
   "metadata": {},
   "source": [
    "### Synthetic data generator\n",
    "\n",
    "This function is brought over from `sensor_generate - commented.py` and produces the\n",
    "longitudinal dataset, train/test splits, and optional diagnostic payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic generator adapted from sensor_generate - commented.py\n",
    "def generate_synthetic_dataset(\n",
    "    N: int = 100,                       # number of base random variables\n",
    "    S: int = 40,                        # number of superposed variables\n",
    "    R: Sequence[int] = (5, 6, 9, 10, 15, 25, 30),  # 1-based indices of relevant superposed vars\n",
    "    AB: Sequence[str] = (\"a\", \"a\", \"b\", \"b\", \"a\", \"b\", \"n\"),  # threshold modes for each relevant var\n",
    "    gp: Sequence[int] = (1, 1, 1, 1, 1, 1, 1),    # or-clause grouping (1-based) - default is all AND\n",
    "    n_cycles: int = 40,                # how many 0-to-pi sinusoid cycles per base signal\n",
    "    H: int = 10,                       # historical timesteps to capture\n",
    "    rseed: int = 1234,                 # RNG seed\n",
    "    train_fr: float = 0.7,             # train/test split fraction\n",
    "    disp: Sequence[int] = (            # time displacements (length S)\n",
    "        0, 0, 0, 0, 10, 6, 0, 10, 1, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 3, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    ),\n",
    "    thresh_a: float = 0.20,            # above threshold (fraction of range)\n",
    "    thresh_b: float = 0.75,            # below threshold (fraction of range)\n",
    "    return_extra: bool = False         # optionally return debug structures\n",
    ") -> Union[Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame],\n",
    "           Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, Dict[str, any]]]:\n",
    "    \"\"\"\n",
    "    Replicates the R scripts synthetic-data generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dset   : pd.DataFrame  full labelled set (chronologically sorted)\n",
    "    train  : pd.DataFrame  training subset\n",
    "    test   : pd.DataFrame  testing  subset (non-overlapping with train)\n",
    "\n",
    "    If `return_extra=True`, the fourth return value is a dict containing\n",
    "    intermediate objects (`SIG_SIN_RND`, `SIG_SUPPOS`, `time_steps`, etc.)\n",
    "    that mirror what the R script stored for inspection.\n",
    "    \"\"\"\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 1) RNG setup and basic helpers\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    np.random.seed(rseed)\n",
    "\n",
    "    # Helper as in R (keep the same convention): degrees → radians.\n",
    "    deg2rad = np.deg2rad\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 2) Build N random-frequency sinusoid vectors (SIG_SIN_RND)\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # Each base vector is formed by concatenating `n_cycles` segments of sinusoids over 90..450 degrees,\n",
    "    # sampled at a random integer step in [1,5].  Then we randomly clip 0..71 samples off the front\n",
    "    # (phase jitter) and record the minimum length across all N vectors so we can equalize later.\n",
    "    sig_sin_rnd: List[np.ndarray] = []\n",
    "    nmin = np.inf                                         # shortest length seen\n",
    "\n",
    "    for _ in range(N):\n",
    "        cycles: List[np.ndarray] = []\n",
    "        for __ in range(n_cycles):\n",
    "            step = np.random.randint(1, 6)                # 1&5 (inclusive)\n",
    "            seq_deg = np.arange(90, 450, step, dtype=float)\n",
    "            cycles.append(np.sin(deg2rad(seq_deg)))\n",
    "        vec = np.concatenate(cycles)\n",
    "\n",
    "        # random phase clipping: drop 071 samples from the front\n",
    "        clip_start = np.random.randint(0, 72)\n",
    "        vec = vec[clip_start:]\n",
    "\n",
    "        # keep track of the shortest vector (needed to align lengths for superposition)\n",
    "        if vec.size < nmin:\n",
    "            nmin = vec.size\n",
    "        sig_sin_rnd.append(vec)\n",
    "\n",
    "    # Equalize lengths and apply a small vertical shift in [0.01, 2.00] to each base vector.\n",
    "    # Vertical shifts prevent degenerate cancellations when summing.\n",
    "    for i, vec in enumerate(sig_sin_rnd):\n",
    "        shift = np.random.randint(1, 201) / 100.0         # 0.01 & 2.00\n",
    "        sig_sin_rnd[i] = vec[:nmin] + shift\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 3) Superpose into S aggregate variables (SIG_SUPPOS)\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # For each of S variables, pick a random base curve and add `nfun` more random base curves,\n",
    "    # then apply a random scale.  This yields richer, partially correlated channels.\n",
    "    nfun = N // S + 1                                      # how many extra curves to add\n",
    "    sig_sup: List[np.ndarray] = []\n",
    "\n",
    "    for _ in range(S):\n",
    "        base = sig_sin_rnd[np.random.randint(0, N)].copy()\n",
    "        for __ in range(nfun):\n",
    "            base += sig_sin_rnd[np.random.randint(0, N)]\n",
    "        scale = (np.random.normal(loc=0.0, scale=0.05) + 0.02) * 100.0\n",
    "        sig_sup.append(base * scale)\n",
    "\n",
    "    sig_sup = np.array(sig_sup)      # shape (S, nmin)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 4) Apply per-variable time displacements \n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # The R script time-shifts certain signals. In this Python version, `SS` is a copy of `sig_sup`.\n",
    "    # Below, the *labeling condition* itself is shifted forward (via `shift_forward_bool`) rather than\n",
    "    # shifting the signal values used as features. This design avoids leakage from trivial copies of\n",
    "    # the target into features when building windows, but is a conscious divergence from the R snippet\n",
    "    # shown in the docstring block (kept as an inert string here).\n",
    "    SS = sig_sup.copy()\n",
    "    \n",
    "    \"\"\"\n",
    "    disp = np.asarray(disp, dtype=int)\n",
    "    for idx_one_based in R:  # R indices are 1-based\n",
    "        idx = idx_one_based - 1\n",
    "        d = disp[idx]\n",
    "        if d > 0:\n",
    "            shifted = np.empty_like(SS[idx])\n",
    "            shifted[:d] = 0\n",
    "            shifted[d:] = SS[idx, :-d]\n",
    "            SS[idx] = shifted\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper to shift a boolean vector forward by d steps\n",
    "    def shift_forward_bool(mask: np.ndarray, d: int) -> np.ndarray:\n",
    "        if d <= 0:\n",
    "            return mask.astype(bool).copy()\n",
    "        out = np.zeros_like(mask, dtype=bool)\n",
    "        out[d:] = mask[:-d]\n",
    "        return out\n",
    "  \n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 5) Build OR-of-ANDs mask of positive outcomes (logic gate)\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # For each relevant series r∈R, compute a boolean condition by thresholding either above 'a',\n",
    "    # below 'b', or between (a,b) depending on AB[i].  Then shift that *condition* by disp[r].\n",
    "    # Within each group g (gp[i]), we AND the conditions (all must hold), and across groups we OR.\n",
    "    # This builds an interpretable DNF-like target, which aligns with logic-parsing approaches to\n",
    "    # longitudinal features highlighted in our BigData'22 paper.\n",
    "    total_len = SS.shape[1]\n",
    "    posg: Dict[int, np.ndarray] = {}\n",
    "    limits: Dict[str, Tuple[float, float]] = {}\n",
    "    \n",
    "    for i, r_one in enumerate(R):\n",
    "        idx = r_one - 1\n",
    "        g = gp[i]\n",
    "        if g not in posg:\n",
    "            posg[g] = np.ones(total_len, dtype=bool)\n",
    "    \n",
    "        vec = sig_sup[idx]                      # <— unshifted baseline series (see note above)\n",
    "        lo, hi = vec.min(), vec.max()\n",
    "        #print(f\" {idx}\")\n",
    "        d = int(disp[idx])\n",
    "        name = f\"V{r_one}TM{d}\"\n",
    "    \n",
    "        if AB[i] == \"a\":\n",
    "            thr = thresh_a * (hi - lo) + lo\n",
    "            cond = vec > thr\n",
    "            limits[name] = (thr, hi)\n",
    "        elif AB[i] == \"b\":\n",
    "            thr = thresh_b * (hi - lo) + lo\n",
    "            cond = vec < thr\n",
    "            limits[name] = (lo, thr)\n",
    "        elif AB[i] == \"n\":\n",
    "            thr_lo = thresh_a * (hi - lo) + lo\n",
    "            thr_hi = thresh_b * (hi - lo) + lo\n",
    "            cond = (vec > thr_lo) & (vec < thr_hi)\n",
    "            limits[name] = (thr_lo, thr_hi)\n",
    "        else:\n",
    "            raise ValueError(\"AB entries must be 'a', 'b', or 'n'.\")\n",
    "    \n",
    "        # Apply time displacement to the condition (not the raw series).\n",
    "        posg[g] &= shift_forward_bool(cond, d)\n",
    "    \n",
    "    # OR across groups to get the final positive mask; also build a simple attribution bitmask.\n",
    "    positive_mask = np.zeros(total_len, dtype=bool)\n",
    "    attrib = np.zeros(total_len, dtype=int)\n",
    "    for g, mask in posg.items():\n",
    "        positive_mask |= mask\n",
    "        attrib += mask.astype(int) * (2 ** (g - 1))\n",
    "\n",
    "    time_steps = np.nonzero(positive_mask)[0]              # 0-based indices where the logic gate fires\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 6) Assemble labelled examples (historical windows of length H+1)\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # For each time t, we pull a *contiguous* block [t-H, …, t] from the S×T matrix (SS),\n",
    "    # flatten it in variable-major order so columns become V_iTM{lag}, and attach label INDC.\n",
    "    # Naming convention: TMH, TM(H-1), …, TM0 (TM = \"time minus\").\n",
    "    def make_examples(indices: np.ndarray, label: bool) -> pd.DataFrame:\n",
    "        records = []\n",
    "        for t in indices:\n",
    "            records.append(SS[:, t - H : t + 1].flatten(order=\"C\"))  # (S*(H+1),)\n",
    "        df = pd.DataFrame(\n",
    "            np.vstack(records),\n",
    "            columns=[f\"V{vi+1}TM{H-j}\"      # TMH, TM(H-1), …, TM0\n",
    "                     for vi in range(S)\n",
    "                     for j in range(0, H+1)]\n",
    "        )\n",
    "        df[\"INDC\"] = label\n",
    "        df[\"X\"] = indices + 1\n",
    "        return df\n",
    "\n",
    "    # Positive rows are those times where the gate fired and we have a full history (t > H).\n",
    "    pos_idx = time_steps[time_steps > H]\n",
    "    # Negatives are the complement (also requiring a full history).\n",
    "    neg_idx = np.setdiff1d(np.arange(total_len), time_steps)\n",
    "    neg_idx = neg_idx[neg_idx > H]\n",
    "\n",
    "    pos_df = make_examples(pos_idx, True)\n",
    "    neg_df = make_examples(neg_idx, False)\n",
    "\n",
    "    # Concatenate and sort chronologically by X.\n",
    "    dset = pd.concat([pos_df, neg_df], ignore_index=True).sort_values(\"X\").reset_index(drop=True)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # 7) Train/test split\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # The split is random without stratification here; prevalence control (if desired) is handled\n",
    "    # upstream by _calibrate_thresholds_to_trfrac in the wrapper.\n",
    "    n_train = int(round(train_fr * len(dset)))\n",
    "    train_sel = np.random.choice(dset.index, size=n_train, replace=False)\n",
    "    dset_train = dset.loc[train_sel].reset_index(drop=True)\n",
    "    dset_test  = dset.drop(train_sel).reset_index(drop=True)\n",
    "\n",
    "    if return_extra:\n",
    "        # Optional debug payload to inspect the generative internals.\n",
    "        extra = dict(\n",
    "            SIG_SIN_RND=sig_sin_rnd,\n",
    "            SIG_SUPPOS=sig_sup,\n",
    "            SS=SS,\n",
    "            time_steps=time_steps,\n",
    "            limits=limits,\n",
    "            attrib=attrib,\n",
    "        )\n",
    "        return dset, dset_train, dset_test, extra\n",
    "    else:\n",
    "        return dset, dset_train, dset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ba385",
   "metadata": {},
   "source": [
    "### Build the Case 1 dataset\n",
    "\n",
    "We reuse the Case 1 parameters (relevant curves, thresholds, displacements, and 70/30 split).\n",
    "The helper below also sets up a metrics log we can extend throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1171d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASE1_DISP = (\n",
    "    0, 0, 0, 0, 10, 0, 0, 10, 1, 0,\n",
    "    0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 3, 0, 0, 0, 0, 7,\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n",
    ")\n",
    "CASE1_RELEVANT = (5, 8, 9, 10, 13, 25, 30)\n",
    "CASE1_AB_MODES = (\"a\", \"a\", \"b\", \"b\", \"a\", \"b\", \"n\")\n",
    "TRUE_FEATURES = [\"V5TM10\",\"V8TM10\",\"V9TM1\",\"V10TM0\",\"V13TM2\",\"V25TM3\",\"V30TM7\"]\n",
    "\n",
    "CASE1_PARAMS = dict(\n",
    "    N=100,\n",
    "    S=40,\n",
    "    R=CASE1_RELEVANT,\n",
    "    AB=CASE1_AB_MODES,\n",
    "    gp=(1, 1, 1, 1, 1, 1, 1),\n",
    "    n_cycles=40,\n",
    "    H=10,\n",
    "    rseed=1234,\n",
    "    train_fr=3437 / 4911,\n",
    "    disp=CASE1_DISP,\n",
    "    thresh_a=0.30,\n",
    "    thresh_b=0.65,\n",
    ")\n",
    "\n",
    "dset, dset_train, dset_test, extra = generate_synthetic_dataset(**CASE1_PARAMS, return_extra=True)\n",
    "\n",
    "def summarize_split(name: str, frame: pd.DataFrame) -> dict:\n",
    "    positives = int(frame[RESP_COL].sum())\n",
    "    total = len(frame)\n",
    "    return {\n",
    "        \"Split\": name,\n",
    "        \"Rows\": total,\n",
    "        \"Positives\": positives,\n",
    "        \"Positive %\": positives / total,\n",
    "    }\n",
    "\n",
    "summary = pd.DataFrame([\n",
    "    summarize_split(\"Full\", dset),\n",
    "    summarize_split(\"Train\", dset_train),\n",
    "    summarize_split(\"Test\", dset_test),\n",
    "])\n",
    "display(summary)\n",
    "\n",
    "metrics_records: List[dict] = []\n",
    "\n",
    "def add_metrics(model: str, view: str, auc_tr: float, auc_te: float,\n",
    "                f1_tr: np.ndarray, f1_te: np.ndarray, notes: str = \"\"):\n",
    "    metrics_records.append(\n",
    "        {\n",
    "            \"Model\": model,\n",
    "            \"View\": view,\n",
    "            \"Train AUC\": auc_tr,\n",
    "            \"Test AUC\": auc_te,\n",
    "            \"Train max F1\": float(np.nanmax(f1_tr)),\n",
    "            \"Test max F1\": float(np.nanmax(f1_te)),\n",
    "            \"Notes\": notes,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e873d",
   "metadata": {},
   "source": [
    "### Peek at the features that actually drive the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_cols = TRUE_FEATURES + [\"INDC\", \"X\"]\n",
    "display(dset[preview_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568a6be",
   "metadata": {},
   "source": [
    "### Relevant sensor curves\n",
    "\n",
    "Each subplot shows one of the seven informative channels with the exact time steps\n",
    "that satisfy the Case 1 logic gate highlighted in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c754bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_sup = np.asarray(extra[\"SIG_SUPPOS\"])\n",
    "time_steps = np.asarray(extra[\"time_steps\"], dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(len(CASE1_RELEVANT), 1, figsize=(12, 2.5 * len(CASE1_RELEVANT)), sharex=True)\n",
    "axes = np.atleast_1d(axes)\n",
    "for ax, var_idx in zip(axes, CASE1_RELEVANT):\n",
    "    series = sig_sup[var_idx - 1]\n",
    "    ax.plot(series, color=\"tab:blue\", linewidth=1)\n",
    "    hits = time_steps[(time_steps >= 0) & (time_steps < series.size)]\n",
    "    ax.scatter(hits, series[hits], color=\"red\", s=8, label=\"event\")\n",
    "    ax.set_ylabel(f\"V{var_idx}\")\n",
    "axes[0].set_title(\"Relevant sensor curves with highlighted events\")\n",
    "axes[-1].set_xlabel(\"Time step\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad6a0a9",
   "metadata": {},
   "source": [
    "### Rectify the data\n",
    "\n",
    "Rectification converts continuous readings into ±1 indicators that capture whether\n",
    "each feature lies within its learned bounds. This is the transformation described\n",
    "in the paper and used before fitting the sparse logistic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2f5ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = organize(dset_train)\n",
    "rt_train, limits = rectify_fast(dset_train, groups, limits=None, sdfilter=None, snap=0.001)\n",
    "rt_test, _ = rectify_fast(dset_test, groups, limits=limits, sdfilter=None, snap=0.001)\n",
    "\n",
    "raw_feature_cols = [c for c in dset_train.columns if c not in EXCLUDE_COLS]\n",
    "raw_train = dset_train[raw_feature_cols].copy()\n",
    "raw_test = dset_test[raw_feature_cols].copy()\n",
    "\n",
    "print(f\"Rectified train shape: {rt_train.shape}, test shape: {rt_test.shape}\")\n",
    "print(f\"Raw train shape: {raw_train.shape}, test shape: {raw_test.shape}\")\n",
    "\n",
    "rectified_preview = rt_train[[*TRUE_FEATURES, RESP_COL]].head()\n",
    "display(rectified_preview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c3a02",
   "metadata": {},
   "source": [
    "### L1 logistic regression (with and without rectification)\n",
    "\n",
    "The 1-SE selection rule keeps the models sparse and mirrors the R implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_outputs = {}\n",
    "\n",
    "sk_rect_pipe, sk_rect_info = sklearn_fit_1se_rectified(\n",
    "    rt_train,\n",
    "    cv=3,\n",
    "    cs=25,\n",
    "    c_lo=-4,\n",
    "    c_hi=4,\n",
    "    tol=1e-3,\n",
    "    max_iter=10000,\n",
    "    solver=\"saga\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "sk_raw_pipe, sk_raw_info = sklearn_fit_1se_raw(\n",
    "    raw_train,\n",
    "    cv=3,\n",
    "    cs=25,\n",
    "    c_lo=-4,\n",
    "    c_hi=4,\n",
    "    tol=1e-3,\n",
    "    max_iter=10000,\n",
    "    solver=\"saga\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "prob_rect_tr, y_rect_tr, auc_rect_tr, f1_rect_tr = evaluate(sk_rect_pipe, rt_train)\n",
    "prob_rect_te, y_rect_te, auc_rect_te, f1_rect_te = evaluate(sk_rect_pipe, rt_test)\n",
    "add_metrics(\"L1 Logistic\", \"Rectified\", auc_rect_tr, auc_rect_te, f1_rect_tr, f1_rect_te)\n",
    "logistic_outputs[\"rectified\"] = dict(train=(prob_rect_tr, y_rect_tr), test=(prob_rect_te, y_rect_te))\n",
    "\n",
    "prob_raw_tr, y_raw_tr, auc_raw_tr, f1_raw_tr = evaluate(sk_raw_pipe, raw_train)\n",
    "prob_raw_te, y_raw_te, auc_raw_te, f1_raw_te = evaluate(sk_raw_pipe, raw_test)\n",
    "add_metrics(\"L1 Logistic\", \"Raw\", auc_raw_tr, auc_raw_te, f1_raw_tr, f1_raw_te)\n",
    "logistic_outputs[\"raw\"] = dict(train=(prob_raw_tr, y_raw_tr), test=(prob_raw_te, y_raw_te))\n",
    "\n",
    "logistic_metrics = pd.DataFrame(\n",
    "    [\n",
    "        {\"View\": \"Rectified\", \"C (1-SE)\": sk_rect_info[\"C_1se\"], \"Train AUC\": auc_rect_tr, \"Test AUC\": auc_rect_te},\n",
    "        {\"View\": \"Raw\", \"C (1-SE)\": sk_raw_info[\"C_1se\"], \"Train AUC\": auc_raw_tr, \"Test AUC\": auc_raw_te},\n",
    "    ]\n",
    ")\n",
    "display(logistic_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view, label in [(\"rectified\", \"Rectified\"), (\"raw\", \"Raw\")]:\n",
    "    train_prob, train_y = logistic_outputs[view][\"train\"]\n",
    "    test_prob, test_y = logistic_outputs[view][\"test\"]\n",
    "    plot_sorted(train_prob, train_y, f\"Case #1 Training Set ({label})\")\n",
    "    plot_sorted(test_prob, test_y, f\"Case #1 Test Set ({label})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282023dc",
   "metadata": {},
   "source": [
    "### Inspect logistic coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd651c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rect_features = rt_train.drop(columns=[RESP_COL]).columns.tolist()\n",
    "coef_rect = coefficient_series_only(sk_rect_pipe, rect_features)\n",
    "plot_coeff_bars(coef_rect, \"Coefficient magnitudes (rectified)\", highlights=TRUE_FEATURES)\n",
    "\n",
    "raw_features = raw_train.drop(columns=[RESP_COL]).columns.tolist()\n",
    "coef_raw = coefficient_series_only(sk_raw_pipe, raw_features)\n",
    "plot_coeff_bars(coef_raw, \"Coefficient magnitudes (raw)\", highlights=TRUE_FEATURES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeef5de9",
   "metadata": {},
   "source": [
    "### Random forest baselines\n",
    "\n",
    "Tree ensembles overfit easily on the untransformed data. Running them on the rectified view\n",
    "demonstrates how much the preprocessing step helps even non-linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = dict(n_estimators=500, max_depth=None, n_jobs=-1, random_state=42, class_weight=\"balanced_subsample\")\n",
    "rf_raw = RandomForestClassifier(**rf_params)\n",
    "rf_raw.fit(raw_train.drop(columns=[RESP_COL]), raw_train[RESP_COL].astype(int))\n",
    "prob_rf_raw_tr, y_rf_raw_tr, auc_rf_raw_tr, f1_rf_raw_tr = evaluate(rf_raw, raw_train)\n",
    "prob_rf_raw_te, y_rf_raw_te, auc_rf_raw_te, f1_rf_raw_te = evaluate(rf_raw, raw_test)\n",
    "add_metrics(\"Random Forest\", \"Raw\", auc_rf_raw_tr, auc_rf_raw_te, f1_rf_raw_tr, f1_rf_raw_te)\n",
    "\n",
    "rf_rect = RandomForestClassifier(**rf_params)\n",
    "rf_rect.fit(rt_train.drop(columns=[RESP_COL]), rt_train[RESP_COL].astype(int))\n",
    "prob_rf_rect_tr, y_rf_rect_tr, auc_rf_rect_tr, f1_rf_rect_tr = evaluate(rf_rect, rt_train)\n",
    "prob_rf_rect_te, y_rf_rect_te, auc_rf_rect_te, f1_rf_rect_te = evaluate(rf_rect, rt_test)\n",
    "add_metrics(\"Random Forest\", \"Rectified\", auc_rf_rect_tr, auc_rf_rect_te, f1_rf_rect_tr, f1_rf_rect_te)\n",
    "\n",
    "rf_metrics = pd.DataFrame(\n",
    "    [\n",
    "        {\"View\": \"Raw\", \"Train AUC\": auc_rf_raw_tr, \"Test AUC\": auc_rf_raw_te},\n",
    "        {\"View\": \"Rectified\", \"Train AUC\": auc_rf_rect_tr, \"Test AUC\": auc_rf_rect_te},\n",
    "    ]\n",
    ")\n",
    "display(rf_metrics)\n",
    "\n",
    "rf_outputs = {\n",
    "    \"raw\": dict(train=(prob_rf_raw_tr, y_rf_raw_tr), test=(prob_rf_raw_te, y_rf_raw_te)),\n",
    "    \"rectified\": dict(train=(prob_rf_rect_tr, y_rf_rect_tr), test=(prob_rf_rect_te, y_rf_rect_te)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24bf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view in [\"raw\", \"rectified\"]:\n",
    "    train_prob, train_y = rf_outputs[view][\"train\"]\n",
    "    test_prob, test_y = rf_outputs[view][\"test\"]\n",
    "    plot_sorted(train_prob, train_y, f\"Random Forest Training ({view})\")\n",
    "    plot_sorted(test_prob, test_y, f\"Random Forest Test ({view})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809fcf08",
   "metadata": {},
   "source": [
    "### Group LASSO style fit\n",
    "\n",
    "`groupyr.LogisticSGLCV` enforces grouped sparsity so each variable's time lags are\n",
    "either kept or dropped together, similar to the group lasso experiments in the R notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groupyr import LogisticSGLCV\n",
    "\n",
    "group_feature_cols = raw_train.drop(columns=[RESP_COL]).columns.tolist()\n",
    "group_labels = [col.split(\"TM\")[0] for col in group_feature_cols]\n",
    "group_index = {name: idx for idx, name in enumerate(sorted(set(group_labels)))}\n",
    "group_ids = np.array([group_index[name] for name in group_labels])\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr = scaler.fit_transform(raw_train[group_feature_cols])\n",
    "Xte = scaler.transform(raw_test[group_feature_cols])\n",
    "ytr = raw_train[RESP_COL].astype(int).to_numpy()\n",
    "yte = raw_test[RESP_COL].astype(int).to_numpy()\n",
    "\n",
    "sgl = LogisticSGLCV(\n",
    "    groups=group_ids,\n",
    "    l1_ratio=0.5,\n",
    "    n_alphas=40,\n",
    "    max_iter=4000,\n",
    "    tol=1e-4,\n",
    "    cv=3,\n",
    "    verbose=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "sgl.fit(Xtr, ytr)\n",
    "prob_sgl_tr = sgl.predict_proba(Xtr)[:, 1]\n",
    "prob_sgl_te = sgl.predict_proba(Xte)[:, 1]\n",
    "prec_tr, rec_tr, _ = precision_recall_curve(ytr, prob_sgl_tr)\n",
    "f1_sgl_tr = 2 * rec_tr * prec_tr / (rec_tr + prec_tr + 1e-9)\n",
    "prec_te, rec_te, _ = precision_recall_curve(yte, prob_sgl_te)\n",
    "f1_sgl_te = 2 * rec_te * prec_te / (rec_te + prec_te + 1e-9)\n",
    "auc_sgl_tr = roc_auc_score(ytr, prob_sgl_tr)\n",
    "auc_sgl_te = roc_auc_score(yte, prob_sgl_te)\n",
    "add_metrics(\n",
    "    \"Logistic Group LASSO\",\n",
    "    \"Raw\",\n",
    "    auc_sgl_tr,\n",
    "    auc_sgl_te,\n",
    "    f1_sgl_tr,\n",
    "    f1_sgl_te,\n",
    "    notes=\"Group-wise penalty via groupyr.LogisticSGLCV\",\n",
    ")\n",
    "\n",
    "print(f\"Best alpha selected: {sgl.alpha_:.4f}\")\n",
    "display(pd.DataFrame([{\"Train AUC\": auc_sgl_tr, \"Test AUC\": auc_sgl_te}]))\n",
    "plot_sorted(prob_sgl_tr, ytr.astype(bool), \"Group LASSO Training (raw view)\")\n",
    "plot_sorted(prob_sgl_te, yte.astype(bool), \"Group LASSO Test (raw view)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cce3e5",
   "metadata": {},
   "source": [
    "### Overall metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c3f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics_records)\n",
    "display(metrics_df.sort_values(\"Test AUC\", ascending=False).reset_index(drop=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
